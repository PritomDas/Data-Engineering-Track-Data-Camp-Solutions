***************************************************Question********************************************************************
In the video, you saw that files are often loaded into a MPP database like Redshift in order to make it available for analysis.

The typical workflow is to write the data into columnar data files. These data files are then uploaded to a storage system and from there, they can be copied into the data warehouse. In case of Amazon Redshift, the storage system would be S3, for example.

The first step is to write a file to the right format. For this exercises you'll choose the Apache Parquet file format.

There's a PySpark DataFrame called film_sdf and a pandas DataFrame called film_pdf in your workspace.
******************************************************************************************************************************


*************************************************Tasks*************************************************************************
1)Write the pandas DataFrame film_pdf to a parquet file called "films_pdf.parquet".
2)Write the PySpark DataFrame film_sdf to a parquet file called "films_sdf.parquet".
*******************************************************************************************************************************


+++++++++++++++++++++++++++++++++++++++++++++++++Solution+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Write the pandas DataFrame to parquet
film_pdf.to_parquet("films_pdf.parquet")

# Write the PySpark DataFrame to parquet
film_sdf.write.parquet("films_sdf.parquet")
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++




??????????????????????????????????????????????Concepts????????????????????????????????????????????????????????????????????
MPP: Massive Parallel processing. While storage and computing power have come long a way in the last several decades, the unfortunate reality is that they havenâ€™t kept up with modern data storage and analysis needs. MPP databases solve this problem by allotting the required processing power onto several different nodes to most efficiently analyze large datasets.
Find more about this here: [https://www.flydata.com/blog/introduction-to-massively-parallel-processing]


Parquet: Apache Parquet is a free and open-source column-oriented data storage format of the Apache Hadoop ecosystem.
Find more about this here: [https://en.wikipedia.org/wiki/Apache_Parquet]
??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
